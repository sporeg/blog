---
layout: post
title: 《数学之美》
date: 2016-03-14
categories: [note]
tags: [entropy,book]
fullview: 
shortinfo: 
description: 《数学之美》笔记
comments: yes
thread: 3
---

在1950年的《Mind》杂志上，图灵提出了图灵测试，留下了一个问题而非答案

============================

对于语音识别和机器翻译，分词是关键

最早采用的是语法规则，显然过于复杂而收获很少，直到提出使用信息论原理进行统计分词，语音识别和机器翻译才有了显著的提升

============================

### 信息熵

一件事情你知道的很少，那么你需要获得更多的信息才能把这件事情搞清楚

也就是说，一件事情发生的概率很低，那么它对应的信息熵就很大

举个栗子

```
32支足球队踢比赛，谁是冠军呢？不知道
正好有个很有经济头脑的同学说他知道，但是他不会告诉你是哪支球队，而是让你问他问题，他回答“是”或“不是”，问一个问题给他1块钱
好吧，问，那么用二分的方法问，那么要问5次把球队问出来
也就是我们花了5块钱
在信息论中，我们要得到的信息是事件x“谁是冠军”，而它的熵是5，即-logp(x)
知道为什么信息熵和log有关了吧？
这是对于等概率事件
那么类似巴西和中国踢呢……显然巴西胜利的概率大，那么就要有加权，H(X)=-p(xi)logp(xi)叠加，这就是信息熵
```

============================

### 冗余

事件X和事件Y的重复部分越多，冗余就越大，那么冗余岂不是很没用？

不是的，如果要保证信息的安全，那么冗余度越大，信息就越安全

```
罗塞塔石碑就是用三种语言写了一件事，最后虽然石碑部分受损，事件依然记录了下来
```

============================

统计中遇到一些从来没见过的事件怎么办，设概率为0？

不是的，可以将已知事件的概率进行按权重压缩，腾出一部分概率给新事件

============================

数字通信中的两个算法

[维特比算法](https://en.wikipedia.org/wiki/Viterbi_algorithm)

用于寻找维特比路径

[BCJR算法](https://en.wikipedia.org/wiki/BCJR_algorithm)

一种定义在网格图上的用来最大化纠错编码的后验概率的算法，主要用于卷积编码

============================

处理数据时，用统计的方法会遇到一些问题

比如一个句子出现的概率怎么算？

直接统计？会死吧= =

那么不如考虑采用信息论的方法

将信息假设为马尔可夫信息链，之后~由于第i个字符只与之前的N个字符相关，统计量就大大缩减

将N增大，准确率就会提高，Google用的是四元

